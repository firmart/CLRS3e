\section{Divide-and-Conquer}

\subsection{The maximum-subarray problem}

\begin{description}
  \descitem{4.1-1} {\itshape What does {\scshape Find-Maximum-Subarray} return when all elements of A are negative?} 
    \begin{ex}
      L'indice du plus grand élément du tableau.
    \end{ex}

  \descitem{4.1-2} {\itshape Write pseudocode for the brute-force method of solving the maximum-subarray problem. Your procedure should run in $\Theta(n^2)$ time.}

    \begin{ex}
      \begin{codebox}
        \Procname{\algo{Brute-Find-Max-Subarray}$(A)$}
        \li $\id{index} \gets  \id{period} \gets -1$
        \li $\id{sum} \gets -\infty$
        \li \For $i=1$ \To \attrib{A}{length} \Do
        \li $\id{tmp-sum} \gets 0$
        \li \For $\id{tmp-period} \gets 1$ \To $\attrib{A}{length} - i+1$ \Do
        \li $\id{tmp-sum} \gets \id{tmp-sum} + A[i + \id{tmp-period}-1]$
        \li \If $\id{tmp-sum} > sum$ \Then
        \li $\id{index} \gets i$
        \li $\id{period} \gets \id{tmp-period} $
        \li $\id{sum} \gets \id{tmp-sum}$ \End \End \End
        \li \Return $(\id{index}, \id{period}, \id{sum})$
      \end{codebox}
    \end{ex}

  \descitem{4.1-3} {\itshape Implement both the brute-force and recursive algorithms for the maximum-subarray problem on your own computer. What problem size $n_0$ gives the crossover point at which the recursive algorithm beats the brute-force algorithm? Then, change the base case of the recursive algorithm to use the brute-force algorithm whenever the problem size is less than $n_0$. Does that change the crossover point?}
    \begin{ex}
        \begin{itemize}
          \item Environ $17$. \footnote{\textit{cf.} le programme \pathx{src/CH04_Divide-and-Conquer/FindMaxSubarray/tst_crossover_pt_1}.}
          \item Le {\itshape crossover point} est devenu $2$. \footnote{\textit{cf.} le programme \pathx{src/CH04_Divide-and-Conquer/FindMaxSubarray/tst_crossover_pt_2}.}
        \end{itemize}
    \end{ex}

  \descitem{4.1-4} {\itshape Suppose we change the definition of the maximum-subarray problem to allow the result to be an empty subarray, where the sum of the values of an empty subarray is $0$. How would you change any of the algorithms that do not allow empty subarrays to permit an empty subarray to be the result?}

    \begin{ex} %TODO:implementation
      \begin{itemize}
        \item Si la somme finale est négative, on retourne $0$ et le tableau vide ;
        \item Initialiser par $0$ la somme.
      \end{itemize}
    \end{ex}

  \descitem{4.1-5} {\itshape Use the following ideas to develop a nonrecursive, linear-time algorithm for the maximum-subarray problem. Start at the left end of the array, and progress toward the right, keeping track of the maximum subarray seen so far. Knowing a maximum subarray of $A[1 \twodots j]$, extend the answer to find a maximum subarray ending at index $j+1$ by using the following observation: a maximum subarray of $A[1 \twodots j + 1]$ is either a maximum subarray of $A[1 \twodots j]$ or a subarray $A[i \twodots j + 1]$, for some $1 \le i \le j + 1$. Determine a maximum subarray of the form $A[i \twodots j + 1]$ in constant time based on knowing a maximum subarray ending at index $j$.}

    \begin{ex} %TODO: implementation
      \begin{codebox}
        \Procname{\algo{Linear-Find-Max-Subarray}$(A)$}
        \li $\id{left} \gets \id{right} \gets 1$
        \li $\id{sum} \gets A[1]$
        \li \For $j = 2$ \To n \Do
        \li $\id{tmp-sum} \gets 0$
        \li \If $A[j] \le 0$ \Then
        \li continue
        \li \Else \If $\id{right} == j - 1$
        \li $\id{right} = j$
        \li $\id{sum} = sum + A[j]$
        \li \Else
        \li $\id{right} \gets j$
        \li $i \gets j - 1$
        \li \While $A[i] > 0$ \Then
        \li $i \gets i - 1$
        \li $\id{left} \gets i + 1$ \End \End \End
        \li \Return \id{sum}
      \end{codebox}
    \end{ex}

\end{description}

\subsection{Strassen’s algorithm for matrix multiplication}

\begin{description}
  \descitem{4.2-1} {\itshape Use Strassen’s algorithm to compute the matrix product \[\begin{pmatrix}
      1 & 3\\
      7 & 5\\
    \end{pmatrix}
    \begin{pmatrix}
      6 & 8\\
      4 & 2\\
    \end{pmatrix}.\] 
  Show your work.}

    \begin{ex}
      Soit $A = \begin{pmatrix}
      1 & 3\\
      7 & 5\\   
    \end{pmatrix} $ et $ B = \begin{pmatrix}
      6 & 8\\
      4 & 2\\
    \end{pmatrix}$, on veut calculer le produit matriciel $AB=C$.
    On {\it divise} les matrices $A$ et $B$ en $4$ sous-matrices: \[\begin{array}{ccccc}
      A_{11} = 1 & A_{12} = 3 & \text{et} & B_{11} = 6& B_{12} = 8\\
      A_{21} = 7 & A_{22} = 5 &&B_{21} = 4& B_{22} = 2\\
    \end{array}
    \] puis on effectue les calcules intermédiaires :
    \begin{alignat*}{7}
    S_1 &= B_{12} - B_{22} &&= 6 &\quad S_6&=B_{11} + B_{22}&&= 8\\
    S_2 &= A_{11} + A_{12} &&= 4 &\quad S_7&=A_{12} - A_{22}&&=-2\\
    S_3 &= A_{21} + A_{22} &&= 12 &\quad S_8&=B_{21}+ B_{22} &&=6\\
    S_4 &= B_{21} - B_{11} &&= -2&\quad S_9&=A_{11}-A_{21}&&=-6\\
    S_5 &= A_{11} + A_{22} &&= 6 &\quad S_{10}&=B_{11}+B_{12}&&=14
    \end{alignat*} et
    \begin{alignat*}{7}
      P_1 &= A_{11}\cdot S_1 &&= 6 &\quad P_4 &= A_{22}\cdot S_4 &&=  -10\\
      P_2 &= S_2\cdot B_{22} &&= 8 &\quad P_5 &= S_5\cdot S_6 &&= 48\\
      P_3 &= S_3\cdot B_{11} &&= 72 &\quad P_6 &= S_7\cdot S_8 &&= -12\\
      &&&&\quad P_7 &= S_9\cdot S_{10} &&= -84
    \end{alignat*}
    On finie par :
    \begin{align*}
      C_{11} &=  P_5 + P_4 - P_2 + P_6 = 18\\
      C_{12} &=  P_1 + P_2 = 14\\
      C_{21} &= P_3 + P_4 = 62\\
      C_{22} &=  P_5 + P_1 - P_3 - P_7 = 66
    \end{align*}
    D'o\`u \[C = \begin{pmatrix}
      18 & 14\\
      62 & 66\\
    \end{pmatrix}.\]
    \end{ex} 

  \descitem{4.2-2} {\itshape Write pseudocode for Strassen’s algorithm.}
    \begin{exrev}
      
    \end{exrev}

  \descitem{4.2-3}{\itshape How would you modify Strassen’s algorithm to multiply $n \times n$ matrices in which $n$ is not an exact power of $2$? Show that the resulting algorithm runs in time $\Theta (n^{\lg 7})$.}
    \begin{exrev}
      
    \end{exrev}
  \descitem{4.2-4}{\itshape What is the largest $k$ such that if you can multiply $3 \times 3$ matrices using $k$ multiplications (not assuming commutativity of multiplication), then you can multiply $n \times n$ matrices in time $o(n^{\lg 7})$? What would the running time of this algorithm be?}

    \begin{exrev}
      
    \end{exrev}
  \descitem{4.2-5}{\itshape V.~Pan has discovered a way of multiplying $68\times 68$ matrices using $132,464$ multiplications, a way of multiplying $70\times 70$ matrices using $143,640$ multiplications, and a way of multiplying $72\times 72$ matrices using $155,424$ multiplications. Which method yields the best asymptotic running time when used in a divide-and-conquer matrix-multiplication algorithm? How does it compare to Strassen’s algorithm?}

    \begin{exrev}
      
    \end{exrev}

  \descitem{4.2-6}{\itshape How quickly can you multiply a $kn\times n$ matrix by an $n\times kn$ matrix, using Strassen’s algorithm as a subroutine? Answer the same question with the order of the input matrices reversed.}

    \begin{ex}   %TODO:add proof (especially for the second case) + diagram
      \begin{itemize}
        \item $kn \times n$ : $\Theta(k^2 n^{\lg 7})$
        \item $n \times kn$ : $\Theta(kn^{\lg 7})$
      \end{itemize}
    \end{ex}

  \descitem{4.2-7} {\itshape Show how to multiply the complex numbers $a + bi$ and $c + di$ using only three multiplications of real numbers. The algorithm should take $a, b, c$, and $d$ as input and produce the real component $ac-bd$ and the imaginary component $ad + bc$ separately.}
    \begin{ex}
      \begin{codebox}% TODO:implement Karatsuba algortihm
        \Procname{\algo{Add-Complex-Number}$(a,b,c,d)$}
        \li $P_1 = ac$
        \li $P_2 = bd$
        \li $P_3 = (a+b)(c+d)$
        \li $\id{real} \gets P_1 - P_2$
        \li $\id{imag} \gets P_3-P_1-P_2$
        \li \Return (\id{real}, \id{imag})
      \end{codebox}

      Ceci est un cas particulier de l'algortihme de Karatsuba\footnote{voir \href{https://en.wikipedia.org/wiki/Karatsuba_algorithm}{Algorithme de Karatsuba} (Wikipedia)} qui a une complexité en temps $\O(n^{\lg 3})$ pour la multiplication entre deux nombres de $n$ chiffres.

    \end{ex}

\end{description}

\subsection{The substitution method for solving recurrences}

\begin{description}
  \descitem{4.3-1} {\itshape Show that the solution of $T(n) = T(n-1) + n$ is $\O(n^2)$.}

    \begin{ex}
      Supposons que $T(n) \le cn^2$. On a 
      \begin{align*}
        T(n) &\le c(n-1)^2 + n\\
        &= c(n^2-2n+1)+n\\
        &= cn^2 - 2cn + c + n\\
        &\le cn^2 \quad\quad \textrm{avec }c \ge \frac{n}{2n-1}.
      \end{align*}
      La fonction $\frac{n}{2n-1}$ étant décroissante, on peut choisir $c = 1$ et $n_0 = 1$ tel que la définition soit vérifiée : 
      \begin{align*}
        T(n) \in \left\{ f(n) : \forall n \ge 1,~ 0 \le f(n) \le n^2 \right\}  \Longrightarrow T(n) \in \O(n^2).
      \end{align*}

    \end{ex}

  \descitem{4.3-2} {\itshape Show that the solution of $T(n) = T(\lceil n/2 \rceil)+1$ is $\O(\lg n)$.}

    \begin{ex}
      Supposons que $T(n) \le c\lg n$ est vraie pour tout $m \le n$ en particulier ${m = \lceil n/2 \rceil}$. Alors 
      \begin{align*}
        T(n) &\le c\lg n/2 + 1\\
        &= c(\lg n - 1) + 1\\
        &= c \lg n -c + 1\\
        &\le c\lg n \quad\quad \forall c \in ]0, 1].
      \end{align*}
    \end{ex}

  \descitem{4.3-3} {\itshape We saw that the solution of $T(n) = 2T(\lfloor n/2 \rfloor) + n$ is $\O(n \lg n)$. Show that the solution of this recurrence is also $\Omega(n \lg n)$. Conclude that the solution is $\Theta(n \lg n)$.}
  \begin{ex} %TODO:first hypothesis
    On suppose que $T(n) \ge c(n+2)\lg(n+2)$. On a 
    \begin{align*}
      T(n) &\ge 2c(\lfloor n/2\rfloor+2)\lg(\lfloor n/2\rfloor+2) + n\\
        &\ge 2c(n/2+1)\lg(n/2+1)+n \\
        &= c(n+2)(\lg(n+2)-1)+n\\
        &= c(n+2)\lg(n+2)-c(n+2)+n\\
        &\ge c(n+2)\lg(n+2) \quad\quad \textrm{avec } c \le 1/3. 
    \end{align*}

    De plus, $ c(n+2)\lg(n+2) \ge cn\lg n$, donc on a bien $T(n) = \Omega(n\lg n)$ pour $c = 1/3$ et $n_0 = 1$ par exemple.

  \end{ex}

  \descitem{4.3-4} {\itshape }

    \begin{exrev} 

    \end{exrev}

  \descitem{4.3-5} {\itshape }

    \begin{exrev}
      
    \end{exrev}

  \descitem{4.3-6} {\itshape }

    \begin{exrev}
      
    \end{exrev}

  \descitem{4.3-7} {\itshape }

    \begin{exrev}
      
    \end{exrev}

  \descitem{4.3-8} {\itshape }

    \begin{exrev}
      
    \end{exrev}

  \descitem{4.3-9} {\itshape }

    \begin{exrev}
      
    \end{exrev}

\end{description}

\subsection{The recursion-tree method for solving recurrences}

\subsection{The master method for solving recurrences}

\begin{description}
 
  \descitem{4.5-1} {\itshape Use the master method to give tight asymptotic bounds for the following recurrences.}
      \begin{Al}
      \item $T(n) = 2T(n/4) + 1$.
      \item $T(n) = 2T(n/4) + \sqrt{n}$.
      \item $T(n) = 2T(n/4) + n$.
      \item $T(n) = 2T(n/4) + n^2$.
     \end{Al}
    
    \begin{ex}
      Dans les quatre cas, on a $\log_b a = \log_4 2 = \frac{1}{2}$.
      \begin{Al}
    \item $f(n) = 1 = \O (n^{\log_4 2-\eps})$ avec $\eps \in ]0, 1]$, donc $T(n) = \Th(\sqrt{n})$.
      \item $f(n) = \sqrt{n} = \Th(\sqrt{n})$, donc $T(n) = \Theta(\sqrt{n}\lg n)$.
    \item $f(n) = n = \Omega(n^{\log_4 2+\eps})$ avec $\eps \in ]0,2]$,  de plus $af(n/b) = n/2 \le cf(n) = cn$ avec $ 1/2 \le c < 1 $, donc $T(n) = \Th(n)$.
      \item $f(n) = n^2 = \Omega(n^{\log_4 2+\eps})$ avec $\eps \in ]0, 14] $, de plus $af(n/b) = n^2/8 \le c f(n)$ avec $ 1/8 \le c < 1$, donc $T(n) = \Th(n^2)$.
      \end{Al}
    \end{ex}

  \descitem{4.5-2} {\itshape Professor Caesar wishes to develop a matrix-multiplication algorithm that is asymptotically faster than Strassen’s algorithm. His algorithm will use the divide-and-conquer method, dividing each matrix into pieces of size $n/4 \times n/4$, and the divide and combine steps together will take $\Th(n^2)$ time. He needs to determine how many subproblems his algorithm has to create in order to beat Strassen’s algorithm. If his algorithm creates a subproblems, then the recurrence for the running time $T(n)$ becomes $T(n) =  aT(n/4) + \Th(n^2)$. What is the largest integer value of a for which Professor Caesar’s algorithm would be asymptotically faster than Strassen’s algorithm?}
    \begin{exrev}
      
    \end{exrev}
 
  \descitem{4.5-3} {\itshape Use the master method to show that the solution to the binary-search recurrence $T(n) = T(n/2) + \Th(1)$ is $T(n) = \Th(\lg n)$. (See Exercise 2.3-5 for a description of binary search.)}

    \begin{ex}
       On a $a=1, b=2$, alors $n^{\lg_b a} = 1$. Comme $f(n) \in \Th(1) = \Th(n^{\lg_b a})$, $T(n) = \Th(\lg n)$.
    \end{ex}
  
  \descitem{4.5-4} {\itshape Can the master method be applied to the recurrence $T(n) = 4T(n/2) + n^2\lg n$ ? Why or why not? Give an asymptotic upper bound for this recurrence.}
    \begin{exrev} % TODO:Tikz recursive tree + methode de substitution
      On compare $n^{\lg_2 4} = n^2$ avec $f(n) = n^2\lg n$. Comme on n'a ni $f(n) = \O(n^{\lg_2 4 - \eps})$ ni $f(n) = \Th(n^2)$  ni $f(n) = \Omega(n^{\lg_2 4 + \eps})$, on ne peut utiliser la {\itshape master method}. Pour trouver une borne supérieure asymptotique, on peut d'abord conjecturer avec la méthode d'arbre récursive puis vérifier l'exactitude avec la méthode de substitution.

      \ul{Arbre récursive} : Supposont que $n = 2^p$ avec $p \in \N$. On a un arbre complet de hauteur $p$ dans lequel chaque n\oe ud est étiqueté par $cn^2\lg n$ et a 4 fils. Chaque niveau $i \in \{0, \ldots, p - 1\}$ a un co\^ut $c4^i\cdot c(\frac{n}{2^i})^2\lg \frac{n}{2^i} = cn^2(p - i)$. Les feuilles sont supposées d'\^etre étiquetées par une constante $\Th(1)$ et il y en a $4^p = n^2$, donc le co\^ut des feuilles est $\Th(n^2)$. On obtient, en sommant les co\^uts de tous les niveaux, une idée sur la borne asymptotique :
      za\begin{align*}
        \sum_{i=0}^{p-1}cn^2(p-k)+ \Th(n^2) &= cn^2\sum_{i=0}^{p-1}(p-k) + \Th(n^2)\\
        &= cn^2(p^2 - \frac{(p-1)p}{2})\\
        &= \Th(n^2\lg^2 n) = \Th\left( (n\lg n)^2 \right).
      \end{align*}

    \ul{Méthode de substitution}:
    \end{exrev}
  
  \item[4.5-5 $\star$] {\itshape Consider the regularity condition $af(n/b) \le cf(n)$ for some constant $c < 1$, which is part of case $3$ of the master theorem. Give an example of constant $a\ge 1$ and $b>1$ and a function $f(n)$ that satisfies all the conditions in case 3 of the master theorem except the regularity condition.}
    \begin{ex}
      Fixons $a = 1, b = 2$ et étudions les fonctions de forme $f(n) = a_n\cdot n$ avec $a_n>0$ qui pourraient vérifier 
      \[\left\{  
          \begin{array}{ll}
            f(n) &= \Omega(n^{\log_2 (1+\eps)}) \quad\textrm{ avec }\eps > 0\\
            f(n/2) &\le cf(n) \quad\textrm{avec } c\ge 1.
          \end{array}
      \right.\]
      La première condition est vérifiée si $a_n$ soit bornée. Pour que la deuxième l'est également, il faut que $a_{n/2} \le 2ca_n$ et que $a_n$ ne soit pas constante. \ul{Intuitivement}, on choisit $a_n = \cos(n)+2$ qui est toujours positive. Par substitution, on obtient l'inégalité suivante \[\frac{\cos(n/2)+2}{2(\cos(n)+2} \le c \tag{1}\] Le maximum du premier membre de l'inégalité $(1)$ est donné par \href{https://www.wolframalpha.com/input/?i=max+(cos(n%2F2)%2B2)%2F(2(cos(n)%2B2)}{Wolfram Alpha} 
et vaut approximativement $1.0303$. Donc on a $1.0303 \le c$, ce qui vérifie la deuxième condition. Ainsi, on ne peut appliquer le {\it master theorem} pour trouver une borne asymptotique inférieure.
    \end{ex}
\end{description}


